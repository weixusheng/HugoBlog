<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="noodp" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
    <title>NNDL-RNN - 小辣鸡的Hugo</title><meta name="author" content="小辣鸡: ">
<meta name="description" content="循环神经网络" />
<meta name="keywords" content="Andrew NNDL" /><meta itemprop="name" content="NNDL-RNN">
<meta itemprop="description" content="循环神经网络"><meta itemprop="datePublished" content="2021-02-16T12:30:00+00:00" />
<meta itemprop="dateModified" content="2021-02-16T12:30:00+00:00" />
<meta itemprop="wordCount" content="3955"><meta itemprop="image" content="http://example.org/logo.png"/>
<meta itemprop="keywords" content="Andrew NNDL," /><meta property="og:title" content="NNDL-RNN" />
<meta property="og:description" content="循环神经网络" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/posts/nndl-21/" /><meta property="og:image" content="http://example.org/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-02-16T12:30:00+00:00" />
<meta property="article:modified_time" content="2021-02-16T12:30:00+00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="http://example.org/logo.png"/>

<meta name="twitter:title" content="NNDL-RNN"/>
<meta name="twitter:description" content="循环神经网络"/>
<meta name="application-name" content="FixIt">
<meta name="apple-mobile-web-app-title" content="FixIt"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#252627"><meta name="msapplication-TileColor" content="#da532c"><link rel="icon" href="/favicon.svg"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="http://example.org/posts/nndl-21/" /><link rel="prev" href="http://example.org/posts/github-5/" /><link rel="next" href="http://example.org/posts/matplotlib-1/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "NNDL-RNN",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "http:\/\/example.org\/posts\/nndl-21\/"
    },"genre": "posts","keywords": "Andrew NNDL","wordcount":  3955 ,
    "url": "http:\/\/example.org\/posts\/nndl-21\/","datePublished": "2021-02-16T12:30:00+00:00","dateModified": "2021-02-16T12:30:00+00:00","publisher": {
      "@type": "Organization",
      "name": "小辣鸡"},"author": {
        "@type": "Person",
        "name": "小辣鸡"
      },"description": ""
  }
  </script></head>
  <body header-desktop="sticky" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('light' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'light' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

    <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
  <div class="header-wrapper">
    <div class="header-title">
      <a href="/" title="小辣鸡的Hugo"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/python.png"
    data-srcset="/python.png, /python.png 1.5x, /python.png 2x"
    data-sizes="auto"
    alt="/python.png"
    title="/python.png" /><span class="header-title-text">小辣鸡的Hugo</span></a><span class="header-subtitle"></span></div>
    <div class="menu">
      <div class="menu-inner"><a class="menu-item" href="/posts/"> 文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/about/"> 关于 </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
            <input type="text" placeholder="Search" id="search-input-desktop">
            <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
              <i class="fas fa-search fa-fw"></i>
            </a>
            <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
              <i class="fas fa-times-circle fa-fw"></i>
            </a>
            <span class="search-button search-loading" id="search-loading-desktop">
              <i class="fas fa-spinner fa-fw fa-spin"></i>
            </span>
          </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
          <i class="fas fa-adjust fa-fw"></i>
        </a>
      </div>
    </div>
  </div>
</header><header class="mobile" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="小辣鸡的Hugo"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/python.png"
    data-srcset="/python.png, /python.png 1.5x, /python.png 2x"
    data-sizes="auto"
    alt="/python.png"
    title="/python.png" /><span class="header-title-text">小辣鸡的Hugo</span></a><span class="header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <div class="menu" id="menu-mobile"><div class="search-wrapper">
          <div class="search mobile" id="search-mobile">
            <input type="text" placeholder="Search" id="search-input-mobile">
            <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
              <i class="fas fa-search fa-fw"></i>
            </a>
            <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
              <i class="fas fa-times-circle fa-fw"></i>
            </a>
            <span class="search-button search-loading" id="search-loading-mobile">
              <i class="fas fa-spinner fa-fw fa-spin"></i>
            </span>
          </div>
          <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
            取消
          </a>
        </div><a class="menu-item" href="/posts/" title="">文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/about/" title="">关于</a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
        <i class="fas fa-adjust fa-fw"></i>
      </a></div>
  </div>
</header>
<div class="search-dropdown desktop">
  <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
  <div id="search-dropdown-mobile"></div>
</div>
<main class="container" page-style="normal"><aside class="toc" id="toc-auto"><h2 class="toc-title">目录</h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom">
    
  </aside>

  <article class="page single"><h1 class="single-title animate__animated animate__flipInX">NNDL-RNN</h1><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><a
  href="/"
  
    title="Author"
  
  
  
    rel=" author"
  
  
  
    class="author"
  
  
><i class="fas fa-user-circle fa-fw"></i>小辣鸡</a></span>&nbsp;<span class="post-category">收录于 <a href="/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><i class="far fa-folder fa-fw"></i>神经网络与深度学习</a></span></div>
      <div class="post-meta-line"><span title=2021-02-16&#32;12:30:00>
            <i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2021-02-16" >2021-02-16</time>
          </span>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 3955 字&nbsp;
        <i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 8 分钟&nbsp;</div>
    </div><div class="details toc" id="toc-static" kept="false">
        <div class="details-summary toc-title">
          <span>目录</span>
          <span><i class="details-icon fas fa-angle-right"></i></span>
        </div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#概念">概念</a></li>
    <li><a href="#为什么要发明循环神经网路">为什么要发明循环神经网路</a></li>
    <li><a href="#循环神经网络的结构及原理">循环神经网络的结构及原理</a></li>
    <li><a href="#rnn实例">RNN实例</a></li>
    <li><a href="#lstm">LSTM</a></li>
    <li><a href="#参考文献">参考文献</a></li>
  </ul>
</nav></div>
      </div><div class="content" id="content"><p>循环神经网络</p>
<p>看了吴老师的视频感觉讲的很快,而且似乎什么都没有听懂&hellip;
所以就去网上找各种大佬总结的博客学了好几天&hellip;感觉就像拼拼图,把RNN的知识点慢慢拼出来了
最后发现李宏毅老师的深度学习讲的很棒!!!种草了嘻嘻</p>
<h1 id="概念">概念</h1>
<p>RNN对具有序列特性的数据非常有效，它能挖掘数据中的时序信息以及语义信息，利用了RNN的这种能力，使深度学习模型在解决语音识别、语言模型、机器翻译以及时序分析等NLP领域的问题时有所突破。
<strong>序列特性</strong>:符合时间顺序，逻辑顺序，或者其他顺序就叫序列特性</p>
<h1 id="为什么要发明循环神经网路">为什么要发明循环神经网路</h1>
<p>我们先来看一个NLP很常见的问题，命名实体识别，举个例子，现在有两句话：</p>
<ul>
<li>第一句话：I like eating apple！（我喜欢吃苹果！）</li>
<li>第二句话：The Apple is a great company！（苹果真是一家很棒的公司！）</li>
</ul>
<p>现在的任务是要给apple打Label，我们都知道第一个apple是一种水果，第二个apple是苹果公司，假设我们现在有大量的已经标记好的数据以供训练模型，当我们使用全连接的神经网络时，我们做法是把apple这个单词的特征向量输入到我们的模型中（如下图），在输出结果时，让我们的label里，正确的label概率最大，来训练模型，但我们的语料库中，有的apple的label是水果，有的label是公司，这将导致，模型在训练的过程中，预测的准确程度，取决于训练集中哪个label多一些，这样的模型对于我们来说完全没有作用。
<strong>问题就出在了我们没有结合上下文去训练模型，而是单独的在训练apple这个单词的label，这也是全连接神经网络模型所不能做到的，于是就有了我们的循环神经网络。</strong></p>
<h1 id="循环神经网络的结构及原理">循环神经网络的结构及原理</h1>
<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/1_.png">
上图就是RNN的结构，我第一次看到这图的第一反应是，不是说好的循环神经网络么，起码得是神经网络啊，神经网络不是有很多球球么，也就是神经元，这RNN咋就这几个球球，不科学啊，看不懂啊！！！！随着慢慢的了解RNN，才发现这图看着是真的清楚，因为RNN的特殊性,如果展开画成那种很多神经元的神经网络，会很麻烦
我们先来讲解一下上面这幅图，首先不要管右边的W，只看X,U,S,V,O，这幅图就变成了之前的全连接神经网络
<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/2_.png">
其中X是一个向量，也就是某个字或词的特征向量，作为输入层，如上图也就是3维向量
U是输入层到隐藏层的参数矩阵，在上图中其维度就是3X4，S是隐藏层的向量，如上图维度就是4
V是隐藏层到输出层的参数矩阵，在上图中就是4X2，O是输出层的向量，在上图中维度为2。
<p>弄懂了RNN结构的左边，那么右边这个W到底是什么啊？把上面那幅图打开之后，是这样的：
<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/3_.png"></p>
<p>举个例子，有一句话是，I love you，那么在利用RNN做一些事情时，比如命名实体识别，上图中的$X_t-1$代表的就是I这个单词的向量，X代表的是love这个单词的向量，$X_t+1$代表的是you这个单词的向量，以此类推，我们注意到，上图展开后，<strong>W一直没有变，W其实是每个时间点之间的权重矩阵</strong>，我们注意到，RNN之所以可以解决序列问题，是因为它可以记住每一时刻的信息，<strong>每一时刻的隐藏层不仅由该时刻的输入层决定，还由上一时刻的隐藏层决定</strong>，公式如下，其中$O_t$代表t时刻的输出, $S_t$代表t时刻的隐藏层的值：
$O_t = g(V\cdot S_t)$
$S_t = f(U\cdot S_t + W\cdot S_t-1)$
值得注意的一点是，在整个训练过程中，每一时刻所用的都是同样的W</p>
<h1 id="rnn实例">RNN实例</h1>
<p>首先对于语义识别中,传统的神经网络缺少记忆
<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/4_.png">
因此加上了两个存储单元$a_1$和$a_2$
<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/12_.png">
假设每个单词的特征向量是二维的，也就是输入层的维度是二维，且隐藏层也假设是二维，输出也假设是二维，所有权重的值都为1且没有偏差且所有激活函数都是线性函数，现在输入一个序列，到该模型中，我们来一步步求解出输出序列
我们可以想象上一时刻的隐藏层的值是被存起来，等下一时刻的隐藏层进来时，上一时刻的隐藏层的值通过与权重相乘，两者相加便得到了下一时刻真正的隐藏层，如图$a_1$, $a_2$可以看做每一时刻存下来的值，当然初始时$a_1$,$a_2$是没有存值的，因此初始值为0</p>
<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/5_.png">
通过公式$S_t = f(U\cdot S_t + W\cdot S_t-1)$
可以计算出来 $1\cdot 1+1\cdot 1+1\cdot 0+1\cdot 0 = 2$
因此绿色的神经元的值为2
再通过公式$O_t = g(V\cdot S_t)$
可以计算出来$2\cdot 1+2\cdot1 = 4$
因此输出值为4
<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/6_.png">
随后绿色神经元将值传给记忆单元,通过相同步骤计算得到
<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/7_.png">
至此，一个完整的RNN结构我们已经经历了一遍，我们注意到，每一时刻的输出结果都与上一时刻的输入有着非常大的关系，如果我们将输入序列换个顺序，那么我们得到的结果也将是截然不同，这就是RNN的特性，可以处理序列数据，同时对序列也很敏感
这样在进行语义识别时,可以根据前句传递下来的"记忆"来判断当前词的含义
<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/8_.png">
<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/9_.png">
<p><strong>注意：上图为同一个RNN在三个不同时间点被分别使用了三次，并非是三个不同的NN</strong></p>
<h1 id="lstm">LSTM</h1>
<p>Long short-term memory(长短期记忆)
如今RNN中大多都会用LSTM来构建模型
因为在基础版本的RNN中，每一时刻的隐藏状态都不仅由该时刻的输入决定，还取决于上一时刻的隐藏层的值，如果一个句子很长，到句子末尾时，它将记不住这个句子的开头的内容详细内容,这就是梯度弥散</p>
<p>LSTM是RNN的一种变体，更高级的RNN，那么它的本质还是一样的，还记得RNN的特点吗，可以有效的处理序列数据，当然LSTM也可以，还记得RNN是如何处理有效数据的吗，是不是每个时刻都会把隐藏层的值存下来，到下一时刻的时候再拿出来用，这样就保证了，每一时刻含有上一时刻的信息，如图，我们把存每一时刻信息的地方叫做Memory_Cell，中文就是记忆细胞，可以这么理解。</p>
<p>打个比喻吧，普通RNN就像一个乞丐，路边捡的，别人丢的，什么东西他都想要，什么东西他都不嫌弃，LSTM就像一个贵族，没有身份的东西他不要，他会精心挑选符合自己身份的物品。这是为什么呢？有没有思考过，原因很简单，乞丐没有选择权，他的能力注定他只能当一个乞丐，因此他没有挑选的权利，而贵族不一样，贵族能力比较强，经过自己的打拼，终于有了地位和身份，所以可以选择舍弃一些低档的东西，这也是能力的凸显。</p>
<p>LSTM和普通RNN正是贵族和乞丐，RNN什么信息它都存下来，因为它没有挑选的能力，<strong>而LSTM不一样，它会选择性的存储信息，因为它能力强，它有门控装置，它可以尽情的选择</strong>。如下图，普通RNN只有中间的Memory_Cell用来存所有的信息，而从下图我们可以看到，LSTM多了三个Gate，也就是三个门，什么意思呢？在现实生活中，门就是用来控制进出的，门关上了，你就进不去房子了，门打开你就能进去，同理，这里的门是用来控制每一时刻信息记忆与遗忘的。
<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/10_.png">
三个门的解释</p>
<ul>
<li>Input_Gate：中文是输入门，在每一时刻从输入层输入的信息会首先经过输入门，输入门的开关会决定这一时刻是否会有信息输入到Memory_Cell</li>
<li>Output_Gate：中文是输出门，每一时刻是否有信息从Memory_Cell输出取决于这一道门</li>
<li>Forget_Gate：中文是遗忘门，每一时刻Memory_Cell里的值都会经历一个是否被遗忘的过程，就是由该门控制的，如果打卡，那么将会把Memory_Cell里的值清除，也就是遗忘掉</li>
</ul>
<p><strong>传递顺序:</strong>
先经过输入门，看是否有信息输入，再判断遗忘门是否选择遗忘Memory_Cell里的信息，最后再经过输出门，判断是否将这一时刻的信息进行输出。
LSTM的结构图，是一个时间点上的内部结构，就是整个工作流程中的其中一个时间点
<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/11_.png"></p>
<p>首先看图中最中间的地方，Cell，我们上面也讲到了Memory_Cell，也就是一个记忆存储的地方，这里就类似于普通RNN的$S_t$，都是用来存储信息的，这里面的信息都会保存到下一时刻，其实标准的叫法应该是$h_t$，因为这里对应神经网络里的隐藏层，所以是hidden的缩写，无论普通RNN还是LSTM其实t时刻的记忆细胞里存的信息，都应该被称为$O_t$。再看最上面的$a$，是这一时刻的输出，也就是类似于普通RNN里的$Z,Z_i,Z_f, Z_o$。最后，我们再来看这四个，这四个相辅相成，才造就了中间的Memory_Cell里的值，你肯恩要问普通RNN里有个$X_t$作为输入，那LSTM的输入在哪？别着急，其实这四个$Z, Z_i,Z_f,Z_o$都有输入向量X_t的参与</p>
<p>其中的运算单元都是激活函数 tanh和sigmoid
$Z = \tanh(W[x_t, h_{t-1}])$<br>
$Z_i = \sigma(W_i [x_t,h_{t-1}])$<br>
$Z_f = \sigma(W_f [x_t, h_{t-1}])$<br>
$Z_o = \sigma(W_o [x_t, h_{t-1}])$<br></p>
<p>其中$Z$是最为普通的输入，可以从上图中看到，$Z$是通过该时刻的输入$Z$和上一时刻存在Memory_Cell里的隐藏层信息$h_{t-1}$向量拼接，再与权重参数向量$W$点积，得到的值经过激活函数tanh最终会得到一个数值，也就是$Z$，注意只有$Z$的激活函数是tanh，因为$Z$是真正作为输入的，其他三个都是门控装置</p>
<p>再来看$Z_i$，input gate的缩写i，所以也就是<strong>输入门的门控装置</strong>，Z_i同样也是通过该时刻的输入$X_t$和上一时刻隐藏状态，也就是上一时刻存下来的信息$h_{t-1}$向量拼接，在与权重参数向量$W_i$点积（注意每个门的权重向量都不一样，这里的下标i代表input的意思，也就是输入门）。</p>
<p>以此类推，就不详细讲解$Z_f,Z_o$了，分别是缩写forget和output的门控装置，原理与上述输入门的门控装置类似。</p>
<p>将$Z, Z_i,Z_f,Z_o$通过activation function,分别得到$g(z),f(z_i),f(z_o),f(z_f)$
<strong>得到的值经过激活函数sigmoid的最终会得到一个0-1之间的一个数值</strong>，用来作为门的控制信号</p>
<ul>
<li>1表示该门完全打开(直接输入值)</li>
<li>0表示该门完全关闭(将旧的值遗忘清除)</li>
</ul>
<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/13_.png">
<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/14_.png">
<h1 id="参考文献">参考文献</h1>
<ul>
<li>[1]<a
  href="https://zhuanlan.zhihu.com/p/123211148?utm_source=qq"
  
  
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>NLP进阶之路</a></li>
<li>[2]<a
  href="https://www.bilibili.com/video/BV1JE411g7XF?p=20"
  
  
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>李弘毅机器学习/深度学习</a></li>
<li>[3]<a
  href="https://sakura-gh.github.io/ML-notes/ML-notes-html/26_Recurrent-Neural-Network-part1.html"
  
  
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>大佬的笔记</a></li>
</ul></div><div class="post-footer" id="post-footer">
  <div class="post-info">
    <div class="post-info-line">
      <div class="post-info-mod">
        <span title=2021-02-16&#32;12:30:00>更新于 2021-02-16</span>
      </div>
      <div class="post-info-license"><span><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
    </div>
    <div class="post-info-line">
      <div class="post-info-md"></div>
      <div class="post-info-share">
        <span></span>
      </div>
    </div>
  </div>

  <div class="post-info-more">
    <section class="post-tags"><i class="fas fa-tags fa-fw"></i>&nbsp;<a href="/tags/Andrew-NNDL/">Andrew NNDL</a></section>
    <section>
      <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
    </section>
  </div>

  <div class="post-nav"><a href="/posts/github-5/" class="prev" rel="prev" title="Git master转为main"><i class="fas fa-angle-left fa-fw"></i>Git master转为main</a>
      <a href="/posts/matplotlib-1/" class="next" rel="next" title="Matplotlib 绘制基本图表">Matplotlib 绘制基本图表<i class="fas fa-angle-right fa-fw"></i></a></div>
</div>
</article></main></div>

    <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
        <i class="fas fa-arrow-up fa-fw"></i>
      </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
        <i class="fas fa-comment fa-fw"></i>
      </a>
    </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/katex/copy-tex.min.css"><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js" defer></script><script type="text/javascript" src="/lib/algoliasearch/algoliasearch-lite.umd.min.js" defer></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js" async defer></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js" defer></script><script type="text/javascript" src="/lib/katex/katex.min.js" defer></script><script type="text/javascript" src="/lib/katex/auto-render.min.js" defer></script><script type="text/javascript" src="/lib/katex/copy-tex.min.js" defer></script><script type="text/javascript" src="/lib/katex/mhchem.min.js" defer></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","editLockTitle":"锁定可编辑代码块","editUnLockTitle":"解锁可编辑代码块","editable":true,"maxShownLines":100},"comment":{},"enablePWA":true,"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"7DXHMB0UGB","algoliaIndex":"hugo","algoliaSearchKey":"4b2bd330d3d2bf31472459d06ecc385f","highlightTag":"em","maxResultLength":50,"noResultsFound":"没有找到结果","snippetLength":50,"type":"algolia"}};</script><script type="text/javascript" src="/js/theme.min.js" defer></script></body>
</html>
