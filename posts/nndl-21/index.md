# NNDL-RNN


循环神经网络
<!--more-->

看了吴老师的视频感觉讲的很快,而且似乎什么都没有听懂...
所以就去网上找各种大佬总结的博客学了好几天...感觉就像拼拼图,把RNN的知识点慢慢拼出来了
最后发现李宏毅老师的深度学习讲的很棒!!!种草了嘻嘻

# 概念
RNN对具有序列特性的数据非常有效，它能挖掘数据中的时序信息以及语义信息，利用了RNN的这种能力，使深度学习模型在解决语音识别、语言模型、机器翻译以及时序分析等NLP领域的问题时有所突破。
**序列特性**:符合时间顺序，逻辑顺序，或者其他顺序就叫序列特性

# 为什么要发明循环神经网路
我们先来看一个NLP很常见的问题，命名实体识别，举个例子，现在有两句话：
 - 第一句话：I like eating apple！（我喜欢吃苹果！）
 - 第二句话：The Apple is a great company！（苹果真是一家很棒的公司！）

现在的任务是要给apple打Label，我们都知道第一个apple是一种水果，第二个apple是苹果公司，假设我们现在有大量的已经标记好的数据以供训练模型，当我们使用全连接的神经网络时，我们做法是把apple这个单词的特征向量输入到我们的模型中（如下图），在输出结果时，让我们的label里，正确的label概率最大，来训练模型，但我们的语料库中，有的apple的label是水果，有的label是公司，这将导致，模型在训练的过程中，预测的准确程度，取决于训练集中哪个label多一些，这样的模型对于我们来说完全没有作用。
**问题就出在了我们没有结合上下文去训练模型，而是单独的在训练apple这个单词的label，这也是全连接神经网络模型所不能做到的，于是就有了我们的循环神经网络。**

# 循环神经网络的结构及原理
<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/1_.png">
上图就是RNN的结构，我第一次看到这图的第一反应是，不是说好的循环神经网络么，起码得是神经网络啊，神经网络不是有很多球球么，也就是神经元，这RNN咋就这几个球球，不科学啊，看不懂啊！！！！随着慢慢的了解RNN，才发现这图看着是真的清楚，因为RNN的特殊性,如果展开画成那种很多神经元的神经网络，会很麻烦
我们先来讲解一下上面这幅图，首先不要管右边的W，只看X,U,S,V,O，这幅图就变成了之前的全连接神经网络
<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/2_.png">
其中X是一个向量，也就是某个字或词的特征向量，作为输入层，如上图也就是3维向量
U是输入层到隐藏层的参数矩阵，在上图中其维度就是3X4，S是隐藏层的向量，如上图维度就是4
V是隐藏层到输出层的参数矩阵，在上图中就是4X2，O是输出层的向量，在上图中维度为2。

弄懂了RNN结构的左边，那么右边这个W到底是什么啊？把上面那幅图打开之后，是这样的：
<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/3_.png">

举个例子，有一句话是，I love you，那么在利用RNN做一些事情时，比如命名实体识别，上图中的$X_t-1$代表的就是I这个单词的向量，X代表的是love这个单词的向量，$X_t+1$代表的是you这个单词的向量，以此类推，我们注意到，上图展开后，**W一直没有变，W其实是每个时间点之间的权重矩阵**，我们注意到，RNN之所以可以解决序列问题，是因为它可以记住每一时刻的信息，**每一时刻的隐藏层不仅由该时刻的输入层决定，还由上一时刻的隐藏层决定**，公式如下，其中$O_t$代表t时刻的输出, $S_t$代表t时刻的隐藏层的值：
$O_t = g(V\cdot S_t)$
$S_t = f(U\cdot S_t + W\cdot S_t-1)$
值得注意的一点是，在整个训练过程中，每一时刻所用的都是同样的W

# RNN实例
首先对于语义识别中,传统的神经网络缺少记忆
<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/4_.png">
因此加上了两个存储单元$a_1$和$a_2$
<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/12_.png">
假设每个单词的特征向量是二维的，也就是输入层的维度是二维，且隐藏层也假设是二维，输出也假设是二维，所有权重的值都为1且没有偏差且所有激活函数都是线性函数，现在输入一个序列，到该模型中，我们来一步步求解出输出序列
我们可以想象上一时刻的隐藏层的值是被存起来，等下一时刻的隐藏层进来时，上一时刻的隐藏层的值通过与权重相乘，两者相加便得到了下一时刻真正的隐藏层，如图$a_1$, $a_2$可以看做每一时刻存下来的值，当然初始时$a_1$,$a_2$是没有存值的，因此初始值为0

<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/5_.png">
通过公式$S_t = f(U\cdot S_t + W\cdot S_t-1)$
可以计算出来 $1\cdot 1+1\cdot 1+1\cdot 0+1\cdot 0 = 2$
因此绿色的神经元的值为2
再通过公式$O_t = g(V\cdot S_t)$
可以计算出来$2\cdot 1+2\cdot1 = 4$
因此输出值为4
<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/6_.png">
随后绿色神经元将值传给记忆单元,通过相同步骤计算得到
<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/7_.png">
至此，一个完整的RNN结构我们已经经历了一遍，我们注意到，每一时刻的输出结果都与上一时刻的输入有着非常大的关系，如果我们将输入序列换个顺序，那么我们得到的结果也将是截然不同，这就是RNN的特性，可以处理序列数据，同时对序列也很敏感
这样在进行语义识别时,可以根据前句传递下来的"记忆"来判断当前词的含义
<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/8_.png">
<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/9_.png">

**注意：上图为同一个RNN在三个不同时间点被分别使用了三次，并非是三个不同的NN**

# LSTM
Long short-term memory(长短期记忆)
如今RNN中大多都会用LSTM来构建模型
因为在基础版本的RNN中，每一时刻的隐藏状态都不仅由该时刻的输入决定，还取决于上一时刻的隐藏层的值，如果一个句子很长，到句子末尾时，它将记不住这个句子的开头的内容详细内容,这就是梯度弥散

LSTM是RNN的一种变体，更高级的RNN，那么它的本质还是一样的，还记得RNN的特点吗，可以有效的处理序列数据，当然LSTM也可以，还记得RNN是如何处理有效数据的吗，是不是每个时刻都会把隐藏层的值存下来，到下一时刻的时候再拿出来用，这样就保证了，每一时刻含有上一时刻的信息，如图，我们把存每一时刻信息的地方叫做Memory_Cell，中文就是记忆细胞，可以这么理解。

打个比喻吧，普通RNN就像一个乞丐，路边捡的，别人丢的，什么东西他都想要，什么东西他都不嫌弃，LSTM就像一个贵族，没有身份的东西他不要，他会精心挑选符合自己身份的物品。这是为什么呢？有没有思考过，原因很简单，乞丐没有选择权，他的能力注定他只能当一个乞丐，因此他没有挑选的权利，而贵族不一样，贵族能力比较强，经过自己的打拼，终于有了地位和身份，所以可以选择舍弃一些低档的东西，这也是能力的凸显。

LSTM和普通RNN正是贵族和乞丐，RNN什么信息它都存下来，因为它没有挑选的能力，**而LSTM不一样，它会选择性的存储信息，因为它能力强，它有门控装置，它可以尽情的选择**。如下图，普通RNN只有中间的Memory_Cell用来存所有的信息，而从下图我们可以看到，LSTM多了三个Gate，也就是三个门，什么意思呢？在现实生活中，门就是用来控制进出的，门关上了，你就进不去房子了，门打开你就能进去，同理，这里的门是用来控制每一时刻信息记忆与遗忘的。
<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/10_.png">
三个门的解释
 - Input_Gate：中文是输入门，在每一时刻从输入层输入的信息会首先经过输入门，输入门的开关会决定这一时刻是否会有信息输入到Memory_Cell
 - Output_Gate：中文是输出门，每一时刻是否有信息从Memory_Cell输出取决于这一道门
 - Forget_Gate：中文是遗忘门，每一时刻Memory_Cell里的值都会经历一个是否被遗忘的过程，就是由该门控制的，如果打卡，那么将会把Memory_Cell里的值清除，也就是遗忘掉

**传递顺序:**
先经过输入门，看是否有信息输入，再判断遗忘门是否选择遗忘Memory_Cell里的信息，最后再经过输出门，判断是否将这一时刻的信息进行输出。
LSTM的结构图，是一个时间点上的内部结构，就是整个工作流程中的其中一个时间点
<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/11_.png">

首先看图中最中间的地方，Cell，我们上面也讲到了Memory_Cell，也就是一个记忆存储的地方，这里就类似于普通RNN的$S_t$，都是用来存储信息的，这里面的信息都会保存到下一时刻，其实标准的叫法应该是$h_t$，因为这里对应神经网络里的隐藏层，所以是hidden的缩写，无论普通RNN还是LSTM其实t时刻的记忆细胞里存的信息，都应该被称为$O_t$。再看最上面的$a$，是这一时刻的输出，也就是类似于普通RNN里的$Z,Z_i,Z_f, Z_o$。最后，我们再来看这四个，这四个相辅相成，才造就了中间的Memory_Cell里的值，你肯恩要问普通RNN里有个$X_t$作为输入，那LSTM的输入在哪？别着急，其实这四个$Z, Z_i,Z_f,Z_o$都有输入向量X_t的参与

其中的运算单元都是激活函数 tanh和sigmoid
$Z = \tanh(W[x_t, h_{t-1}])$<br>
$Z_i = \sigma(W_i [x_t,h_{t-1}])$<br>
$Z_f = \sigma(W_f [x_t, h_{t-1}])$<br>
$Z_o = \sigma(W_o [x_t, h_{t-1}])$<br>

其中$Z$是最为普通的输入，可以从上图中看到，$Z$是通过该时刻的输入$Z$和上一时刻存在Memory_Cell里的隐藏层信息$h_{t-1}$向量拼接，再与权重参数向量$W$点积，得到的值经过激活函数tanh最终会得到一个数值，也就是$Z$，注意只有$Z$的激活函数是tanh，因为$Z$是真正作为输入的，其他三个都是门控装置

再来看$Z_i$，input gate的缩写i，所以也就是**输入门的门控装置**，Z_i同样也是通过该时刻的输入$X_t$和上一时刻隐藏状态，也就是上一时刻存下来的信息$h_{t-1}$向量拼接，在与权重参数向量$W_i$点积（注意每个门的权重向量都不一样，这里的下标i代表input的意思，也就是输入门）。

以此类推，就不详细讲解$Z_f,Z_o$了，分别是缩写forget和output的门控装置，原理与上述输入门的门控装置类似。

将$Z, Z_i,Z_f,Z_o$通过activation function,分别得到$g(z),f(z_i),f(z_o),f(z_f)$
**得到的值经过激活函数sigmoid的最终会得到一个0-1之间的一个数值**，用来作为门的控制信号
 - 1表示该门完全打开(直接输入值)
 - 0表示该门完全关闭(将旧的值遗忘清除)

<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/13_.png">
<img loading="lazy" src="https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/21/14_.png">

# 参考文献
 - [1][NLP进阶之路](https://zhuanlan.zhihu.com/p/123211148?utm_source=qq)
 - [2][李弘毅机器学习/深度学习](https://www.bilibili.com/video/BV1JE411g7XF?p=20)
 - [3][大佬的笔记](https://sakura-gh.github.io/ML-notes/ML-notes-html/26_Recurrent-Neural-Network-part1.html)
