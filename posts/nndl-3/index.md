# NNDL-回归模型


线性回归&逻辑回归
<!--more-->

# 线性回归–Linear Regression
线性回归模型可应用与监督学习(supervised learning)预测问题，如房价预测，疾病预测…….
对线性回归的直观解释: 找到一个线性函数能尽可能拟合已给出的点
![](https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/4/1.png)

理论解释:
## 影响因素
$$X=[x_1,x_2,x_3,.......,x_m]T$$
$x_i$代表影响结果的一个特征(feature), y ∈ R 
当X与结果y属于线性关系时，即可得到线性回归假设(hypotheses)：
$$h_θ(X)=θ_1x_1+θ_2x_2+.......+θ_mx_m+b$$
写成向量形式即为：
$$h_Θ(X)=Θ^TX+b$$

## 损失函数
构造损失函数（损失函数就是用来表现预测与实际数据的差距程度）:
cost function（损失函数）: 
$$J(\Theta)={1\over2}\sum_i^{smaples}[h_\Theta(X)-y]$$

利用梯度下降方法找到一组 $$\Theta=[θ_1,θ_2,....,θ_m]$$ 
最小化J(Θ)
在得到Θ向量后，将 $$X_predict$$  代入 $$h_Θ(X)=Θ^TX$$ 中即可得到  $$y_predict $$

### 最小二乘法
[CSDN解释](https://blog.csdn.net/qq_41598072/article/details/83984299)

标准定义:
![](https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/4/least_square_1.jpg)
用一个具体的例子来说明:
小明是跑运输的，跑1公里需要6块，跑2公里需要5块（那段时间刚好油价跌了），跑3公里需要7块，跑4公里需要10块，请问跑5公里需要多少块？
如果我们有初中数学基础，应该会自然而然地想到用线性方程组来做，对吧。
$y = \beta_1+\beta_2x$

这里假定x是公里数，y是运输成本（β1和β2是要求的系数）。我们把上面的一组数据代入得到这么几个方程：
$\beta_1+1\beta_2 = 6$

$\beta_1+2\beta_2 = 5$

$\beta_1+3\beta_2 = 7$

$\beta_1+4\beta_2 = 10$

如果存在这样的β1和β2，让所有的数据（x，y）=（1,6），（2,5），（3,7），（4，10）都能满足的话，那么解答就很简单了，β1+5β2就是5公里的成本，对吧。
但遗憾的是，这样的β1和β2是不存在的，上面的方程组很容易，你可以把前面两个解出来得到一组β1和β2，后面两个也解出来同样得到一组β1和β2。这两组β1和β2是不一样的。
形象地说，就是你找不到一条直线，穿过所有的点，因为他们不在一条直线上。如下图：
![](https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/4/least_square_4.jpg)

可是现实生活中，我们就希望能找到一条直线，虽然不能满足所有条件，但能近似地表示这个趋势，或者说，能近似地知道5公里的运输成本，这也是有意义的。

现实生活当中，有很多这样的例子，想起以前在某公司上班的时候，CEO说我们研发部做事有个问题：一个研发任务，要求三个月做完，因为周期太短，完成不了，就干脆不做，这显然是不对的，要尽全力，哪怕三个月完成了80%，或者最终4个月完成，总比不作为的好。

其实最小二乘法也是这样，要尽全力让这条直线最接近这些点，那么问题来了，怎么才叫做最接近呢？直觉告诉我们，这条直线在所有数据点中间穿过，让这些点到这条直线的误差之和越小越好。这里我们用方差来算更客观。也就是说，把每个点到直线的误差平方加起来：
$$S( \beta_1, \beta_2) = [6-(\beta_1+1\beta_2)]^{2} +[5-(\beta_1+2\beta_2)]^{2}+
  [7-(\beta_1+3\beta_2)]^{2} + $$
$$[10-(\beta_1+4\beta_2)]^{2}$$

（如果上面的四个方程都能满足，那么S的值显然为0，这是最完美的，但如果做不到完美，我们就让这个S越小越好）

接下来的问题就是，如何让这个S变得最小。这里有一个概念，就是求偏导数。这里我想提一下，在培训的过程中，我发现机器学习的数学基础课程当中，微积分是大家印象最深刻的，而且也最容易理解：比如导数就是求变化率，而偏导数则是当变量超过一个的时候，对其中一个变量求变化率。如果这个概念也忘了，可以参考我在深度学习回答里那个王小二卖猪的例子。这里就不细讲了：

要让S取得最小值（或最大值，但显然这个函数没有最大值，自己琢磨一下），那么S对于β1和β2分别求偏导结果为0，用一个直观的图来表示：
![](https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/4/least_square_6.jpg)

我们看到这条曲线，前半部分是呈下降的趋势，也就是变化率（导数）为负的，后半部分呈上升的趋势，也就是变化率（导数）为正，那么分界点的导数为0，也就是取得最小值的地方。这是一个变量的情况，对于多个变量的情况，要让S取得最小值，那最好是对β1和β2分别求导（对β1求导的时候，把β2当常量所以叫求偏导），值为0：
$$\frac{\partial S}{\partial\beta_1} = 0 = 8\beta_1+20\beta_2-56$$

$$\frac{\partial S}{\partial\beta_2} = 0 = 20\beta_1+60\beta_2-154$$

看到这个我们就熟悉了，两个变量，刚好有两个方程式，初中学过，那么很容易得出：
$\beta_1 = 3.5 \beta_2 = 1.4$ 
其实也就意味着
$y = 3.5+1.4x$
这个函数也就是我们要的直线，这条直线虽然不能把那些点串起来，但它能最大程度上接近这些点。也就是说5公里的时候，成本为3.5+1.4x5=10.5块，虽然不完美，但是很接近实际情况。

# 逻辑回归–Logistic Regression
逻辑回归模型应用于监督学习分类问题，暂且考虑二分类(如0,1分类)问题
直观解释：得到一条函数将不同类别的分开，如图:
![](https://tronwei-1254020584.cos.ap-beijing.myqcloud.com/NNDL/4/2.png)

假设
$X=[x_1,x_2,x_3,.......,x_m]$
$Θ=[θ_1,θ_2,θ_3,.......θ_m]$
定义hypotheses(也就是预测预测结果 $\check{y}$) 

$hΘ(X)=\frac{1}{1+exp(−Θ^TX)}$

$$\left \lbrace\begin{matrix}
    h_\Theta(X)>0.5, set h_\Theta(X) = 1\\
    h_\Theta(X)<0.5, set h_\Theta(X) = 0
\end{matrix} \right.$$

cost functions:
$$J(\Theta) = \left\{\begin{matrix}
    -log(h_\Theta(x^i)), if y^i = 1\\
    -log(1-h_\Theta(x^i)), if y^i = 0
\end{matrix}\right.$$

统一起来即为:
$$J(\theta) = \sum_{i}{samples}[y^{i}log(h_\Theta)(x^i)+((1-y^i)log(1-h_\Theta(x^i))]$$

同样是**用到梯度下降优化参数**,得到优化后的参数$$\Theta = [\Theta_1,\Theta_2,...\Theta_m]$$后,将$$X_predict$$ 代入 $$h_\Theta(X)$$
即可得到对该X对应的分类

[参考博客](https://blog.csdn.net/weixin_37140379/article/details/82289704)
